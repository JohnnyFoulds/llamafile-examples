{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01-02 : H2O \n",
    "\n",
    "## References\n",
    "\n",
    "- [h2oai/h2o-danube3-500m-chat-GGUF](h2oai/h2o-danube3-500m-chat-GGUF)\n",
    "- [foulds/h2o-danube3-500m-chat-llamafile](https://huggingface.co/foulds/h2o-danube3-500m-chat-llamafile/tree/main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Download Llamafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -o \"../models/h2o-danube3-500m.llamafile\" \"https://huggingface.co/foulds/h2o-danube3-500m-chat-llamafile/resolve/main/h2o-danube3-500m-chat-Q5_K_M.llamafile?download=true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x \"../models/h2o-danube3-500m.llamafile\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import stat\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Llamafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"../models/h2o-danube3-500m.llamafile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executable started in the background.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"build\":1500,\"commit\":\"a30b324\",\"function\":\"server_cli\",\"level\":\"INFO\",\"line\":2869,\"msg\":\"build info\",\"tid\":\"34364426944\",\"timestamp\":1721307795}\n",
      "{\"function\":\"server_cli\",\"level\":\"INFO\",\"line\":2872,\"msg\":\"system info\",\"n_threads\":4,\"n_threads_batch\":-1,\"system_info\":\"AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \",\"tid\":\"34364426944\",\"timestamp\":1721307795,\"total_threads\":8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Apple Metal GPU support successfully loaded\n",
      "llama_model_loader: loaded meta data with 28 key-value pairs and 147 tensors from h2o-danube3-500m-chat-Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = danube3-500M\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 16\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 1536\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 4096\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 100000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 2\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
      "llama_model_loader: - kv  27:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   33 tensors\n",
      "llama_model_loader: - type q5_K:   97 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 1536\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 16\n",
      "llm_load_print_meta: n_rot            = 96\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 96\n",
      "llm_load_print_meta: n_embd_head_v    = 96\n",
      "llm_load_print_meta: n_gqa            = 2\n",
      "llm_load_print_meta: n_embd_k_gqa     = 768\n",
      "llm_load_print_meta: n_embd_v_gqa     = 768\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 4096\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 100000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 513.59 M\n",
      "llm_load_print_meta: model size       = 350.69 MiB (5.73 BPW) \n",
      "llm_load_print_meta: general.name     = danube3-500M\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: SEP token        = 2 '</s>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: CLS token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.17 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =   280.03 MiB, (  280.09 /  5461.34)\n",
      "llm_load_tensors: offloading 16 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 16/17 layers to GPU\n",
      "llm_load_tensors:      Metal buffer size =   280.02 MiB\n",
      "llm_load_tensors:        CPU buffer size =   350.69 MiB\n",
      ".........................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 100000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1\n",
      "ggml_metal_init: picking default device: Apple M1\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/foulds/.llamafile/v/0.8.9/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  =  5726.63 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.13 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =    29.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    65.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 518\n",
      "llama_new_context_with_model: graph splits = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"function\":\"initialize\",\"level\":\"INFO\",\"line\":489,\"msg\":\"initializing slots\",\"n_slots\":1,\"tid\":\"34364426944\",\"timestamp\":1721307796}\n",
      "{\"function\":\"initialize\",\"level\":\"INFO\",\"line\":498,\"msg\":\"new slot\",\"n_ctx_slot\":512,\"slot_id\":0,\"tid\":\"34364426944\",\"timestamp\":1721307796}\n",
      "{\"function\":\"server_cli\",\"level\":\"INFO\",\"line\":3090,\"msg\":\"model loaded\",\"tid\":\"34364426944\",\"timestamp\":1721307796}\n",
      "{\"function\":\"server_cli\",\"hostname\":\"0.0.0.0\",\"level\":\"INFO\",\"line\":3213,\"msg\":\"HTTP server listening\",\"port\":\"8081\",\"tid\":\"34364426944\",\"timestamp\":1721307796}\n",
      "{\"function\":\"validate_model_chat_template\",\"level\":\"ERR\",\"line\":478,\"msg\":\"The chat template comes with this model is not yet supported, falling back to chatml. This may cause the model to output suboptimal responses\",\"tid\":\"34364426944\",\"timestamp\":1721307796}\n",
      "{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1659,\"msg\":\"all slots are idle and system prompt is empty, clear the KV cache\",\"tid\":\"34364426944\",\"timestamp\":1721307796}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama server listening at http://192.168.1.237:8081\n",
      "llama server listening at http://127.0.0.1:8081\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Construct the full path to the executable\n",
    "executable_path = os.path.join(cwd, model_file)\n",
    "\n",
    "# Ensure the file has execute permissions\n",
    "if not os.access(executable_path, os.X_OK):\n",
    "    st = os.stat(executable_path)\n",
    "    os.chmod(executable_path, st.st_mode | stat.S_IEXEC)\n",
    "\n",
    "# Define the arguments separately\n",
    "arguments = ['--port', '8081', '--host', '0.0.0.0', '--nobrowser']\n",
    "\n",
    "# Start the executable in the background\n",
    "llamafile_process = subprocess.Popen(['bash', executable_path] + arguments)\n",
    "print(\"Executable started in the background.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8081/v1\",\n",
    "    api_key = \"sk-no-key-required\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The sky is blue because it reflects sunlight. When sunlight hits the Earth's atmosphere, it scatters and refracts, causing the blue color of the sky. This is due to the scattering of shorter wavelengths of light, such as blue, by molecules and particles in the atmosphere. The scattering of longer wavelengths, like red and orange, is less pronounced, resulting in the blue color we see.</s>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"LLaMA_CPP\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant. Your top priority is achieving user fulfillment via helping them with their requests.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Why is the sky blue\"}\n",
    "    ]\n",
    ")\n",
    "clear_output()\n",
    "Markdown(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Summarization Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "This text is about semantic search, a search engine technology that interprets the meaning of words and phrases. It is designed to improve the quality of search results by matching search intent to semantic meaning with the help of technologies such as machine learning and artificial intelligence.</s>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "What is this text about?\n",
    "\n",
    "TEXT:\n",
    "\n",
    "Semantic search is a search engine technology that interprets the meaning \n",
    "of words and phrases. The results of a semantic search will return content\n",
    "matching the meaning of a query, as opposed to content that literally \n",
    "matches words in the query.\n",
    "\n",
    "Semantic search is a set of search engine capabilities, which includes \n",
    "understanding words from the searcher’s intent and their search context.\n",
    "\n",
    "This type of search is intended to improve the quality of search results\n",
    "by interpreting natural language more accurately and in context. Semantic\n",
    "search achieves this by matching search intent to semantic meaning with\n",
    "the help of technologies such as machine learning and artificial\n",
    "intelligence.\n",
    "\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"LLaMA_CPP\",\n",
    "    max_tokens=100,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "clear_output()\n",
    "Markdown(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "Assistant: Of course! The customer complaint you're responding to is about the Vodabucks store with no customer support and managers who don't understand your problems. The customer is unhappy with the lack of assistance and the high prices they are paying for the vodabucks app. They also mention that the special on the vodabucks app does not correlate with the voucher they received, causing them to pay more than they would have without using their voucher system.</s>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "What is this customer complaint about?\n",
    "\n",
    "Problems with the Vodabucks store with NO help and even managers that \n",
    "does not understand your problem. No resolution of my problems. The \n",
    "special on the vodabucks app does not correlate with the sms and voucher\n",
    "received causing me to pay way more than what I would have paid without\n",
    "using their voucher system\n",
    "\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"LLaMA_CPP\",\n",
    "    max_tokens=256,\n",
    "    temperature=0,\n",
    "    top_p=0,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an customer support agent tasked with classifying complaints. You respond only with a single sentence after reviewing the text.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "clear_output()\n",
    "Markdown(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Stop Llamafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llamafile_process.terminate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamafile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
